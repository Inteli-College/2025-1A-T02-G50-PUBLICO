## Summary for Possible Publication

The educational inclusion of deaf individuals in Brazil faces significant challenges due to communication barriers between Brazilian Sign Language (Libras) and Portuguese. This study proposes a real-time automatic translation system from Libras to text and/or speech, combining computer vision and artificial intelligence techniques to recognize static gestures (e.g., numbers and letters) with future integration of facial expressions. The methodology employs Convolutional Neural Networks (CNNs) for visual pattern extraction and Recurrent Neural Networks (RNNs) or Transformers to capture temporal sequences in dynamic gestures, enhanced by MediaPipe for real-time keypoint detection. Results demonstrate high accuracy in basic recognition tasks, overcoming limitations of existing solutions such as lack of support for regional variations and facial expressions. Designed specifically for the Brazilian educational context, the system shows potential to reduce school dropout rates and promote social inclusion. Its scalability and low-cost architecture may enable widespread adoption in public schools, with future applications in healthcare and employment sectors.