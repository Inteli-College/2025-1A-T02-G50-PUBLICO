# Estado da Arte

O estado da arte para esse projeto de tradução de línguas de sinais, especialmente da Língua Brasileira de Sinais (Libras), tem avançado significativamente nos últimos anos, impulsionado pelo desenvolvimento de técnicas de visão computacional e deep learning, assim como outras tecnicas de pesquisas cientificas e projetos desenvolvido por outros pesquisadores. No entanto, apesar dos progressos, ainda existem desafios significativos a serem superados, especialmente no que diz respeito voltado à educação do ensino médio brasileiro, mas como também à precisão, tempo de resposta e inclusão de nuances culturais e regionais. Nesta seção, revisamos as pesquisas e soluções mais avançadas existentes, identificamos lacunas e destacamos como o presente projeto busca contribuir para o avanço do campo.

O projeto de tradução simultânea da Língua Brasileira de Sinais para surdos e ouvintes na educação do ensino médio brasileiro depende fortemente de técnicas de visão computacional para capturar e interpretar gestos e expressões faciais. As redes neurais convolucionais (CNNs) têm sido amplamente utilizadas para reconhecer gestos estáticos, como os sinais que representam letras e números. Essas redes são eficazes para extrair características visuais, como a forma das mãos e a direção dos movimentos, e têm alcançado altas taxas de precisão em tarefas de reconhecimento de gestos. No entanto, a Libras não se limita a gestos estáticos; ela inclui movimentos dinâmicos e expressões faciais que são essenciais para a comunicação. Para lidar com essas complexidades, redes neurais recorrentes (RNNs) e Transformers têm sido empregados para capturar sequências temporais e traduzir gestos dinâmicos em tempo real.

Além disso, ferramentas como MediaPipe e OpenCV hão de ser utilizadas para detectar pontos-chave das mãos e do rosto, fornecendo dados de entrada para modelos de IA. O MediaPipe, em particular, tem se destacado por sua eficiência em tempo real, permitindo a detecção de gestos e expressões faciais com baixa latência. Então, essas ferramentas são especialmente úteis para projetos que exigem processamento em tempo real, como é o caso da tradução de Libras.

Várias soluções têm sido desenvolvidas para a tradução automática de línguas de sinais, tanto no âmbito acadêmico quanto comercial. Projetos, por exemplo, como o [Sign-Language-Detection](https://github.com/SomyanshAvasthi/Sign-Language-Detection-using-CNN-Architecture.git) e o [Sign Language Interpreter (Deep Learning)](https://github.com/harshbg/Sign-Language-Interpreter-using-Deep-Learning.git) demonstram a eficácia de CNNs e RNNs no reconhecimento de gestos estáticos e dinâmicos. Esses projetos alcançaram taxas de precisão acima de 95% em tarefas de reconhecimento de gestos, mas ainda enfrentam desafios, como a falta de suporte para regionalismos e expressões faciais e como também o limite da taxa etária e qual seria o tamanho e o padrão do usuário, pois cada pessoa possa ter um tamanho diferente de outro e a leitura pode sofrer com diferenças de resultados, que são essenciais para uma tradução precisa da Libras.

No âmbito comercial, aplicativos como o [Hand Talk](https://www.handtalk.me/br/) oferecem tradução de Libras para português, mas são limitados em sua capacidade de reconhecer gestos dinâmicos e expressões faciais. Além disso, muitas dessas soluções não são especificamente voltadas para o contexto educacional no contidiano brasileiro, o que limita sua aplicabilidade em salas de aula.

É importante validar que, apesar dos avanços, ainda existem lacunas significativas na literatura e nas soluções existentes. Uma das principais lacunas é a falta de suporte para regionalismos da Libras. Como a Libras varia regionalmente, um sistema de tradução automática deve ser capaz de reconhecer e interpretar essas variações para garantir uma tradução precisa. Além disso, muitas soluções existentes não consideram as expressões faciais, que são parte integrante da comunicação em Libras. Outro desafio é a latência; muitas soluções não funcionam em tempo real, o que limita sua aplicabilidade em situações que exigem comunicação rápida e eficiente, como em salas de aula ou reuniões.

Outra lacuna importante é a falta de foco no contexto educacional. A maioria das soluções existentes é genérica e não foi desenvolvida especificamente para atender às necessidades de estudantes surdos e de deficientes auditivos no ensino médio, sobretudo no Brasil. Isso inclui a falta de integração com materiais didáticos e plataformas de ensino, bem como a dificuldade de acesso a essas tecnologias por escolas públicas e instituições com menos recursos.

Portanto, o presente projeto busca preencher essas lacunas ao desenvolver um sistema de tradução automática de Libras que é preciso, acessível e culturalmente sensível. Diferente de muitas soluções existentes, o sistema proposto é especificamente voltado para o contexto educacional do ensino médio brasileiro, com foco em numerais, letras e em sequência as expressões faciais. Além disso, o sistema será desenvolvido com base em técnicas de visão computacional (CNNs) e IA (RNNs e Transformers) que permitem o reconhecimento de gestos estáticos e dinâmicos, bem como a integração de expressões faciais.

Logo, projeto tem o potencial de gerar impactos significativos tanto no âmbito técnico quanto no social. No âmbito técnico, o desenvolvimento de um sistema de tradução automática de Libras que considera também as expressões faciais representa um avanço importante no campo da visão computacional e IA aplicadas à inclusão social. No âmbito social, o sistema pode melhorar o acesso à educação para estudantes surdos, reduzir a evasão escolar e promover a inclusão de profissionais surdos nos vestibulares e por consequência no mercado de trabalho.





