# State of the Art

The state of the art for this sign language translation project, particularly focusing on Brazilian Sign Language (Libras), has advanced significantly in recent years, driven by developments in computer vision techniques, deep learning, and other scientific research methods, as well as projects developed by other researchers. However, despite this progress, significant challenges remain, especially concerning Brazilian high school education, as well as accuracy, response time, and the inclusion of cultural and regional nuances. In this section, we review the most advanced existing research and solutions, identify gaps, and highlight how this project aims to contribute to the field's advancement.

The project for real-time translation of Brazilian Sign Language for deaf and hearing individuals in Brazilian high school education heavily relies on computer vision techniques to capture and interpret gestures and facial expressions. Convolutional Neural Networks (CNNs) have been widely used to recognize static gestures, such as signs representing letters and numbers. These networks are effective at extracting visual features, such as hand shape and movement direction, and have achieved high accuracy rates in gesture recognition tasks. However, Libras is not limited to static gestures; it includes dynamic movements and facial expressions that are essential for communication. To address these complexities, Recurrent Neural Networks (RNNs) and Transformers have been employed to capture temporal sequences and translate dynamic gestures in real time.

Additionally, tools such as MediaPipe and OpenCV are used to detect key points on the hands and face, providing input data for AI models. MediaPipe, in particular, has stood out for its real-time efficiency, enabling low-latency detection of gestures and facial expressions. These tools are especially useful for projects requiring real-time processing, such as Libras translation.

Several solutions have been developed for automatic sign language translation, both in academic and commercial contexts. Projects such as [Sign-Language-Detection](https://github.com/SomyanshAvasthi/Sign-Language-Detection-using-CNN-Architecture.git) and [Sign Language Interpreter (Deep Learning)](https://github.com/harshbg/Sign-Language-Interpreter-using-Deep-Learning.git) demonstrate the effectiveness of CNNs and RNNs in recognizing static and dynamic gestures. These projects have achieved accuracy rates above 95% in gesture recognition tasks but still face challenges, such as a lack of support for regional variations and facial expressions, as well as limitations regarding user age, size, and patternâ€”since each person may have different physical characteristics, which can affect recognition accuracy. These factors are crucial for precise Libras translation.

In the commercial sphere, apps like [Hand Talk](https://www.handtalk.me/br/) offer Libras-to-Portuguese translation but are limited in their ability to recognize dynamic gestures and facial expressions. Moreover, many of these solutions are not specifically designed for the Brazilian educational context, which limits their applicability in classrooms.

It is important to note that, despite advancements, significant gaps remain in the literature and existing solutions. One major gap is the lack of support for regional variations in Libras. Since Libras varies by region, an automatic translation system must recognize and interpret these variations to ensure accurate translation. Additionally, many existing solutions do not account for facial expressions, which are integral to Libras communication. Another challenge is latency; many solutions do not operate in real time, limiting their applicability in situations requiring fast and efficient communication, such as classrooms or meetings.

Another critical gap is the lack of focus on the educational context. Most existing solutions are generic and were not developed specifically to meet the needs of deaf and hard-of-hearing high school students, particularly in Brazil. This includes the absence of integration with teaching materials and educational platforms, as well as limited access to these technologies in public schools and under-resourced institutions.

Therefore, this project seeks to address these gaps by developing an automatic Libras translation system that is accurate, accessible, and culturally sensitive. Unlike many existing solutions, the proposed system is specifically tailored to the Brazilian high school educational context, focusing on numerals, letters, and subsequently, facial expressions. Furthermore, the system will be developed using computer vision techniques (CNNs) and AI (RNNs and Transformers) to enable the recognition of static and dynamic gestures, as well as the integration of facial expressions.

Thus, this project has the potential to generate significant impacts both technically and socially. On the technical side, the development of an automatic Libras translation system that also incorporates facial expressions represents an important advancement in the field of computer vision and AI applied to social inclusion. On the social side, the system could improve access to education for deaf students, reduce school dropout rates, and promote the inclusion of deaf professionals in university entrance exams and, consequently, in the job market.





