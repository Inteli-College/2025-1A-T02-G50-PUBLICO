# Theoretical Foundations

The automatic translation project of Brazilian Sign Language (Libras) to Portuguese using computer vision and artificial intelligence (AI) is an interdisciplinary field combining linguistics, computer vision, AI, and above all, social inclusion. These theoretical foundations are essential to support the development of the proposed system, ensuring it is technically viable, culturally sensitive, and socially impactful.

First, it's important to remember that Libras is a visual-spatial language with its own grammatical structure and unique characteristics that distinguish it from spoken languages. Unlike Portuguese (an oral-auditory language), Libras uses hand signs, facial expressions, and body movements to convey meaning. Therefore, its grammar includes elements like hand configuration, articulation points, movement patterns, and non-manual markers (such as facial expressions and head movements). These elements are fundamental to Libras communication and must be considered in developing the automatic translation system. Additionally, Libras has regional variations, meaning the same sign may have different meanings across Brazil. Initially, the system will recognize basic Libras signs, but through iterative development, it will progressively achieve accurate translation of expressions, regional variations, and thus become culturally sensitive.

Furthermore, computer vision—a computer science field enabling machines to "see" and interpret images/videos—will be used to capture and process hand gestures and facial expressions composing Libras signs. Techniques like filtering, segmentation, and edge detection enhance captured image quality, facilitating sign recognition. Convolutional Neural Networks (CNNs) are particularly effective for recognizing visual patterns like hand shapes and movement directions, making them ideal for both static and dynamic gesture recognition essential to automatic Libras translation.

Moreover, artificial intelligence (AI), especially deep learning, plays a crucial role in sign language recognition and translation. Models like **Recurrent Neural Networks (RNNs)** and **Transformers** excel at capturing temporal sequences (e.g., dynamic gestures involving motion over time). RNNs are particularly useful for processing sequential data like sign language videos, while Transformers—advanced models—have proven efficient for translation and natural language processing (NLP) tasks. These models can be trained on large datasets to recognize and translate signs with high accuracy. NLP techniques can then translate captured signs into text or speech, enabling the system to serve hearing individuals unfamiliar with Libras.

Thus, as an assistive technology—tools helping people with disabilities overcome limitations and fully participate in society—this proposed Libras-to-Portuguese translation system exemplifies technology that can promote accessibility and inclusion, particularly in Brazilian high schools. By facilitating communication between deaf and hearing individuals, the system can improve access to essential services beyond education (e.g., healthcare and employment), reducing barriers that limit full societal participation for deaf individuals.








